{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereafer I will import some of the python libraries I will use in the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os, sys\n",
    "from functools import partial\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am defining the logger, I will use it to print some information about the execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    stream=sys.stdout,\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    datefmt=\"%d/%m/%Y %I:%M:%S%p\")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Challenge - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the solution of the challange and some explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.featureExtractor import FeatureExtraxtor, plotHistogram\n",
    "from source.model import MLmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am loading the dataset from the web:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://data.dft.gov.uk/road-accidents-safety-data/DfTRoadSafety_Accidents_2014.zip\"\n",
    "dataset = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the featureExtractor.py module I defined a class that contains all the method that I used to preprocess the data. Now I am instatiating an element of this class. In the constructor it needs the path of the csv file with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myFE = FeatureExtraxtor(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will generate the features. I used different methods for each column or group of columns, you can read the docstrings and the comment in featureExtractor.py for all the details: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = myFE.getFeatures()\n",
    "features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundTruth = myFE.getGroundTruth()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging all togheter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.merge(groundTruth, features).set_index('Accident_Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for feature in features.set_index('Accident_Index').columns:\n",
    "    plotHistogram(feature, train_set).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I will define and train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "I am defining X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_set.drop(columns='GROUND_TRUTH')\n",
    "y = train_set['GROUND_TRUTH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am splitting both the features and the ground truth into a train and test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.1,\n",
    "                                                    stratify=y,\n",
    "                                                    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, I've tested several tree-based model. I defined them in a configuration file called model_settings.yaml, now I am importing that in the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join('source', 'model_settings.yaml'), 'r', encoding=\"utf-8\") as handler:\n",
    "    model_params = yaml.load(handler, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am defining an instance of a MLmodel object, a class defined in the model.py module (in the source folder) that contain several useful methods for dealing with tree based classification models.\n",
    "I've choosen this class of models because they offer a great compromise between simplicity and quality of the predictios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClassificationModel = MLmodel(model_params['LightGBM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am launching the optimization routine based on gridsearch or randomsearch (to be chosen in the configuration file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myClassificationModel.optimize(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I am fitting the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myClassificationModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KPIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am assembling the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = y_train.reset_index()\n",
    "results_test = y_test.reset_index()\n",
    "\n",
    "results_train['Score'] = myClassificationModel.predict(X_train)\n",
    "results_train['Prediction'] = (results_train['Score'] > 0.5).astype(int)\n",
    "\n",
    "results_test['Score'] = myClassificationModel.predict(X_test)\n",
    "results_test['Prediction'] = (results_test['Score'] > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, I'll print some quality metrics, to check the quality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\nTrain set:\")\n",
    "\n",
    "myClassificationModel.compute_model_gof_kpis(predictions = results_train,\n",
    "                                             true_class_name='GROUND_TRUTH',\n",
    "                                             pred_class_name='Prediction',\n",
    "                                             pred_score_name='Score',)\n",
    "\n",
    "print(\"\\n\\nTest Set:\")\n",
    "myClassificationModel.compute_model_gof_kpis(predictions = results_test,\n",
    "                                             true_class_name='GROUND_TRUTH',\n",
    "                                             pred_class_name='Prediction',\n",
    "                                             pred_score_name='Score',)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the features importances. Tree based models offer a simple method to check the relative importances of the features, so I think they are a great choice to easily get some extra-insight about the goodness of the chosen features. In this case, I made several attempts and I deleted some features which importance were always 0 or close to 0. There are also automatic routines defined in scikit-learn to do this task (for example sklearn.feature_selection.RFE) but they require a lot of time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClassificationModel.plotFeaturesImportances(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. What was approach taken (e.g. algorithms and tools)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used pandas for data manipulation, plotly for visual data exploration, lightGBM and some scikit-learn utilities for the model. I've chosen a gradient boosted algorithm because they are pretty easy to optimize, they give good predictions (expecially for medium sized datasets, like in this case) and it give the possibility to easily infer the feature importance ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. What were the main challenges?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the dataset, because I was unfamiliar with the problem. Another challenge was to deal with the short time available, that made me to take many shortcuts and to rely heavily to already done code (lack of customization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. What insight did you gain from working with the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found that the location of the accident (expressed in my feature set with the latitude/longitude pair) is by far the best predictor of our ground truth. I guess that it's due to the fact that there are places where there are more often police officers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. How useful is the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is pretty useful in predicting the presence of the police officer, less useful in predicting his/her absence (on the test set, the number of false negatives is twice as big as the number of true negatives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e. What might you do differently if you had more time/resource? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereafter some ideas to improve the model with more time available:\n",
    "\n",
    "    1. I could extract different features from LAT/LON pair, using some geocoding API\n",
    "    \n",
    "    2. I would explore in more detail the categorical features, that are not used in this example\n",
    "    \n",
    "    3. I would try to increase the number of feature, using other data sources (in the website http://data.dft.gov.uk/road-accidents-safety-data/ there are other files that I could try to explore)\n",
    "    \n",
    "    4. I would try to increase the dimension of the dataset (adding data from previous years)\n",
    "    \n",
    "    5. I would try different models \n",
    "    \n",
    "    6. I would perform a better hyperparameter optimization (using a gridsearch on a larger hyperparameter space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uk_accidents",
   "language": "python",
   "name": "uk_accidents"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
